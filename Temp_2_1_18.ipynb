{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from utilities import my_callbacks\n",
    "from utilities import data_helper\n",
    "import optparse\n",
    "import sys\n",
    "import math\n",
    "\n",
    "np.set_printoptions(threshold=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using features: None\n",
      "{'X': 162058, 'S': 52415, '-': 2449404, 'O': 30440}\n",
      "Total vocabulary size in the whole dataset: 4\n",
      "['-', 'O', 'S', 'X', '0']\n"
     ]
    }
   ],
   "source": [
    "vocab = data_helper.load_all(filelist=\"final_data/wsj.all\")\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading entity-gird for pos and neg documents...\n"
     ]
    }
   ],
   "source": [
    "print(\"loading entity-gird for pos and neg documents...\")\n",
    "\n",
    "X_train_1, X_train_0, E = data_helper.load_and_numberize_Egrid_with_Feats(\"final_data/wsj.train\", \n",
    "        perm_num = 20, maxlen=2000, window_size=6, vocab_list=vocab, emb_size=100)\n",
    "\n",
    "X_dev_1, X_dev_0, E    = data_helper.load_and_numberize_Egrid_with_Feats(\"final_data/wsj.dev\", \n",
    "        perm_num = 20, maxlen=2000, window_size=6, E = E, vocab_list=vocab, emb_size=100)\n",
    "\n",
    "X_test_1, X_test_0, E    = data_helper.load_and_numberize_Egrid_with_Feats(\"final_data/wsj.test\", \n",
    "        perm_num = 20, maxlen=2000, window_size=6, E = E, vocab_list=vocab, emb_size=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".....................................\n",
      "Num of traing pairs: 23744\n",
      "Num of dev pairs: 2678\n",
      "Num of test pairs: 20411\n",
      ".....................................\n"
     ]
    }
   ],
   "source": [
    "num_train = len(X_train_1)\n",
    "num_dev   = len(X_dev_1)\n",
    "num_test  = len(X_test_1)\n",
    "#assign Y value\n",
    "y_train_1 = [1] * num_train \n",
    "y_dev_1 = [1] * num_dev \n",
    "y_test_1 = [1] * num_test \n",
    "\n",
    "print('.....................................')\n",
    "print(\"Num of traing pairs: \" + str(num_train))\n",
    "print(\"Num of dev pairs: \" + str(num_dev))\n",
    "print(\"Num of test pairs: \" + str(num_test))\n",
    "#print(\"Num of permutation in train: \" + str(opts.p_num)) \n",
    "#print(\"The maximum in length for CNN: \" + str(opts.maxlen))\n",
    "print('.....................................')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#randomly shuffle the training data\n",
    "np.random.seed(113)\n",
    "np.random.shuffle(X_train_1)\n",
    "np.random.seed(113)\n",
    "np.random.shuffle(X_train_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_propagation(X_positive, X_negative, vocab, E, print_ = False):\n",
    "    \"\"\"\n",
    "    Implements forward propagation of Neural coherence model\n",
    "    \n",
    "    Arguments:\n",
    "    X_positive -- A Placeholder for positive document\n",
    "    X_negative -- A Placeholder for negative document\n",
    "    vocab -- Vocabulary list of entire entity grid list\n",
    "    E -- initialized values for embedding matrix\n",
    "    print_ -- Whether size of the variables to be printed\n",
    "    \n",
    "    Returns: \n",
    "    out_positive -- Coherence Score for positive document\n",
    "    out_negative -- Coherence Score for negative document\n",
    "    parameters -- a dictionary of tensors containing trainable parameters\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    ## Placeholders\n",
    "    #X_positive = tf.placeholder(tf.int32, shape = [None, 2000]) #opts.maxlen=2000\n",
    "    #X_negative = tf.placeholder(tf.int32, shape = [None, 2000]) #opts.maxlen=2000\n",
    "    \n",
    "\n",
    "    ## First Layer of NN: Transform each grammatical role in the grid into distributed representation - a real valued vector\n",
    "    \n",
    "    \n",
    "    #Shared embedding matrix\n",
    "    #W_embedding = tf.get_variable(\"W_embedding\", [len(vocab), 100], initializer = tf.contrib.layers.xavier_initializer()) #embedding matrix \n",
    "    E = np.float32(E) # DataType of E is float64, which is not in list of allowed values in conv1D. Allowed DataType: float16, float32\n",
    "    W_embedding = tf.get_variable(\"W_embedding\", initializer = E) #embedding matrix \n",
    "   \n",
    "    \n",
    "    #Look up layer\n",
    "    \n",
    "    #for positive document\n",
    "    embedding_positive = tf.nn.embedding_lookup(W_embedding, X_positive)\n",
    "    \n",
    "    #for negative document\n",
    "    embedding_negative = tf.nn.embedding_lookup(W_embedding, X_negative)\n",
    "\n",
    "\n",
    "    ## Second Layer of NN: Convolution Layer\n",
    "    \n",
    "    \n",
    "    #shared filter and bias\n",
    "    w_size = 6       #filter_size\n",
    "    emb_size = 100   #embedding_size \n",
    "    nb_filter = 150  #num_filters \n",
    "\n",
    "    filter_shape = [w_size, emb_size, nb_filter]\n",
    "\n",
    "    #W_conv_layer_1 = tf.get_variable(\"W_conv_layer_1\", shape = filter_shape, initializer = tf.contrib.layers.xavier_initializer(seed = 0)) #filter for covolution layer 1\n",
    "    W_conv_layer_1 = tf.get_variable(\"W_conv_layer_1\", shape = filter_shape, initializer = tf.contrib.layers.xavier_initializer(seed = 100)) #filter for covolution layer 1\n",
    "    b_conv_layer_1 =  tf.get_variable(\"b_conv_layer_1\", shape=[nb_filter], initializer = tf.constant_initializer(0.0))  #bias for convolution layer 1\n",
    "\n",
    "    \n",
    "       \n",
    "    #1D Convolution for positive document\n",
    "    conv_layer_1_positive = tf.nn.conv1d(embedding_positive, W_conv_layer_1, stride=1, padding=\"VALID\")  #embedding and W_conv_layer_1 both are 3D matrix\n",
    "    conv_layer_1_with_bias_positive = tf.nn.bias_add(conv_layer_1_positive, b_conv_layer_1)    \n",
    "    h_conv_layer_1_positive = tf.nn.relu(conv_layer_1_with_bias_positive, name=\"relu_conv_layer_1_positive\") # Apply nonlinearity\n",
    "    \n",
    "    \n",
    "    #1D Convolution for negative document\n",
    "    conv_layer_1_negative = tf.nn.conv1d(embedding_negative, W_conv_layer_1, stride=1, padding=\"VALID\")  #embedding and W_conv_layer_1 both are 3D matrix\n",
    "    conv_layer_1_with_bias_negative = tf.nn.bias_add(conv_layer_1_negative, b_conv_layer_1)    \n",
    "    h_conv_layer_1_negative = tf.nn.relu(conv_layer_1_with_bias_negative, name=\"relu_conv_layer_1_negative\") # Apply nonlinearity\n",
    "\n",
    "    \n",
    "\n",
    "    ## Third Layer of NN: Pooling Layer\n",
    "    \n",
    "    \n",
    "    #1D Pooling for positive document\n",
    "    m_layer_1_positive = tf.nn.pool(h_conv_layer_1_positive, window_shape = [6], strides = [6], pooling_type = 'MAX', padding=\"VALID\")\n",
    "\n",
    "    #1D Pooling for negative document\n",
    "    m_layer_1_negative = tf.nn.pool(h_conv_layer_1_negative, window_shape = [6], strides = [6], pooling_type = 'MAX', padding=\"VALID\")\n",
    "\n",
    "\n",
    "\n",
    "    ## Fourth Layer of NN: Fully Connected Layer\n",
    "    \n",
    "    #Dropout Early [As Dat Used]\n",
    "    \n",
    "    #for positive document\n",
    "    #drop_out_early_positive = tf.nn.dropout(m_layer_1_positive, keep_prob=0.5)\n",
    "    \n",
    "    #for negative document\n",
    "    #drop_out_early_negative = tf.nn.dropout(m_layer_1_negative, keep_prob=0.5)\n",
    "    \n",
    "    \n",
    "    #Flatten\n",
    "    \n",
    "    #for positive document\n",
    "    flatten_positive = tf.contrib.layers.flatten(m_layer_1_positive)\n",
    "    #flatten_positive = tf.contrib.layers.flatten(drop_out_early_positive)\n",
    "    \n",
    "    #for negative document\n",
    "    flatten_negative = tf.contrib.layers.flatten(m_layer_1_negative)\n",
    "    #flatten_negative = tf.contrib.layers.flatten(drop_out_early_negative)\n",
    "    \n",
    "\n",
    "    #Dropout\n",
    "    \n",
    "    #for positive document\n",
    "    drop_out_positive = tf.nn.dropout(flatten_positive, keep_prob=0.5, seed=100)\n",
    "    \n",
    "    #for negative document\n",
    "    drop_out_negative = tf.nn.dropout(flatten_negative, keep_prob=0.5, seed=100)\n",
    "\n",
    "    \n",
    "    \n",
    "    # Coherence Scoring\n",
    "    v_fc_layer = tf.get_variable(\"v_fc_layer\", shape = [49800, 1], initializer = tf.contrib.layers.xavier_initializer(seed = 100)) #Weight matrix for final layer\n",
    "    b_fc_layer =  tf.get_variable(\"b_fc_layer\", shape=[1], initializer = tf.constant_initializer(0.0))  #bias for final layer\n",
    "\n",
    "    \n",
    "    \n",
    "    #for positive document\n",
    "    #out_positive = tf.contrib.layers.fully_connected(drop_out_positive, num_outputs = 1, activation_fn=None)\n",
    "    #out_positive = tf.sigmoid(out_positive)\n",
    "    out_positive = tf.add(tf.matmul(drop_out_positive, v_fc_layer), b_fc_layer)\n",
    "    \n",
    "    #for negative document\n",
    "    #out_negative = tf.contrib.layers.fully_connected(drop_out_negative, num_outputs = 1, activation_fn=None)\n",
    "    #out_negative = tf.sigmoid(out_negative)\n",
    "    out_negative = tf.add(tf.matmul(drop_out_negative, v_fc_layer), b_fc_layer)\n",
    "    \n",
    "    \n",
    "    parameters = {\"W_embedding\": W_embedding,\n",
    "                  \"W_conv_layer_1\": W_conv_layer_1,\n",
    "                  \"b_conv_layer_1\": b_conv_layer_1,\n",
    "                  \"v_fc_layer\": v_fc_layer,\n",
    "                  \"b_fc_layer\": b_fc_layer}\n",
    "    \n",
    "    \n",
    "    if(print_):\n",
    "        print(\"Layer (type)          Output Shape\")\n",
    "        print(\"_________________________________________\")\n",
    "        print(\"\\nInputLayer:\")\n",
    "        print(\"X_positive           \",   X_positive.shape)\n",
    "        print(\"X_negative           \",   X_negative.shape)\n",
    "        print(\"\\nEmbedding Layer:\")\n",
    "        print(\"Embedding Matrix     \",   W_embedding.shape)\n",
    "        print(\"Embedding Positive   \",   embedding_positive.shape)\n",
    "        print(\"Embedding Negative   \",   embedding_negative.shape)\n",
    "        print(\"\\nConvolution 1D Layer:\")\n",
    "        print(\"Filter Shape         \",   W_conv_layer_1.shape)\n",
    "        print(\"Conv Positive        \",   h_conv_layer_1_positive.shape)\n",
    "        print(\"Conv Negative        \",   h_conv_layer_1_negative.shape)\n",
    "        print(\"\\nMax Pooling 1D Layer:\")\n",
    "        print(\"MaxPool Positive     \",   m_layer_1_positive.shape)\n",
    "        print(\"MaxPool Negative     \",   m_layer_1_negative.shape)\n",
    "        print(\"\\nFlatten Layer: \")\n",
    "        print(\"Flatten Positive     \",   flatten_positive.shape)\n",
    "        print(\"Flatten Negative     \",   flatten_negative.shape)\n",
    "        print(\"\\nDropout Layer: \")\n",
    "        print(\"Dropout Positive     \",   drop_out_positive.shape)\n",
    "        print(\"Dropout Negative     \",   drop_out_negative.shape)\n",
    "        print(\"\\nFully Connected Layer:\")\n",
    "        print(\"FC Positive          \",   out_positive.shape)\n",
    "        print(\"FC Negative          \",   out_negative.shape)\n",
    "        \n",
    "        \n",
    "    \n",
    "    return out_positive, out_negative, parameters\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ranking_loss(pos, neg):\n",
    "    \"\"\"\n",
    "    Implements the ranking objective.\n",
    "    \n",
    "    Arguments:\n",
    "    pos -- score for positive document batch\n",
    "    neg -- score for negative document batch\n",
    "    \n",
    "    Returns:\n",
    "    Average ranking loss for the batch  \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    loss = tf.maximum(1.0 + neg - pos, 0.0) \n",
    "    #print(loss)\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mini_batches(X, Y, m, mini_batch_size = 32):\n",
    "    \"\"\"\n",
    "    Creates minibatches.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- Positive Documents\n",
    "    Y -- Negative Documents\n",
    "    m -- Number of Documents\n",
    "    mini_batch_size -- Size of each mini batch. \n",
    "    \n",
    "    Returns:\n",
    "    list of mini batches from the positive and negative documents.\n",
    "    \n",
    "    \"\"\"\n",
    "        \n",
    "    mini_batches = []\n",
    "    \n",
    "    num_complete_minibatches = math.floor(m/mini_batch_size) \n",
    "    \n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = X[k * mini_batch_size : k * mini_batch_size + mini_batch_size, :]\n",
    "        mini_batch_Y = Y[k * mini_batch_size : k * mini_batch_size + mini_batch_size, :]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = X[num_complete_minibatches * mini_batch_size : m, :]\n",
    "        mini_batch_Y = Y[num_complete_minibatches * mini_batch_size : m, :]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create Placeholders\n",
    "X_positive = tf.placeholder(tf.int32, shape = [None, 2000]) #Placeholder for positive document\n",
    "X_negative = tf.placeholder(tf.int32, shape = [None, 2000]) #Placeholder for negative document\n",
    "\n",
    "# Forward propagation\n",
    "score_positive, score_negative, parameters = forward_propagation(X_positive, X_negative, vocab, E, print_=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost function:\n",
    "cost = ranking_loss(score_positive, score_negative)\n",
    "\n",
    "# Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer that minimizes the cost.\n",
    "#optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)\n",
    "#optimizer = tf.train.RMSPropOptimizer(learning_rate=0.001, decay=0.0, momentum=0.9, epsilon=1e-8).minimize(cost)\n",
    "\n",
    "\n",
    "## Using keras RMSProp\n",
    "\n",
    "W_embedding = parameters[\"W_embedding\"]\n",
    "W_conv_layer_1 = parameters[\"W_conv_layer_1\"]\n",
    "b_conv_layer_1 = parameters[\"b_conv_layer_1\"]\n",
    "v_fc_layer = parameters[\"v_fc_layer\"]\n",
    "b_fc_layer = parameters[\"b_fc_layer\"]\n",
    "optimizer = tf.keras.optimizers.RMSprop().get_updates(cost, [W_embedding, W_conv_layer_1, b_conv_layer_1, v_fc_layer, b_fc_layer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  0 :   0.997883\n",
      "Iteration  1 :   0.981161\n",
      "Iteration  2 :   1.00119\n",
      "Iteration  3 :   0.931452\n",
      "Iteration  4 :   0.989699\n",
      "Iteration  5 :   0.995366\n",
      "Iteration  6 :   0.909427\n",
      "Iteration  7 :   1.0299\n",
      "Iteration  8 :   0.924531\n",
      "Iteration  9 :   0.971643\n",
      "******************* End of an epoch ******************************\n",
      "Hello\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[20411,2000,100]\n\t [[Node: embedding_lookup = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, _class=[\"loc:@W_embedding\"], validate_indices=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](W_embedding/read, _arg_Placeholder_0_0)]]\n\nCaused by op 'embedding_lookup', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/tasnim/tensorflow/lib/python3.5/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/tasnim/tensorflow/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/tasnim/tensorflow/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 478, in start\n    self.io_loop.start()\n  File \"/home/tasnim/tensorflow/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/tasnim/tensorflow/lib/python3.5/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/tasnim/tensorflow/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/tasnim/tensorflow/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/tasnim/tensorflow/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/tasnim/tensorflow/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/tasnim/tensorflow/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/tasnim/tensorflow/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 281, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/tasnim/tensorflow/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 232, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/tasnim/tensorflow/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 397, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/tasnim/tensorflow/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/tasnim/tensorflow/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/tasnim/tensorflow/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/tasnim/tensorflow/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/tasnim/tensorflow/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-86-cad23cf9c1e7>\", line 6, in <module>\n    score_positive, score_negative, parameters = forward_propagation(X_positive, X_negative, vocab, E, print_=False)\n  File \"<ipython-input-74-314e7ae23068>\", line 36, in forward_propagation\n    embedding_positive = tf.nn.embedding_lookup(W_embedding, X_positive)\n  File \"/home/tasnim/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/embedding_ops.py\", line 328, in embedding_lookup\n    transform_fn=None)\n  File \"/home/tasnim/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/embedding_ops.py\", line 150, in _embedding_lookup_and_transform\n    result = _clip(_gather(params[0], ids, name=name), ids, max_norm)\n  File \"/home/tasnim/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/embedding_ops.py\", line 54, in _gather\n    return array_ops.gather(params, ids, name=name)\n  File \"/home/tasnim/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/array_ops.py\", line 2486, in gather\n    params, indices, validate_indices=validate_indices, name=name)\n  File \"/home/tasnim/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 1834, in gather\n    validate_indices=validate_indices, name=name)\n  File \"/home/tasnim/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/tasnim/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"/home/tasnim/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[20411,2000,100]\n\t [[Node: embedding_lookup = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, _class=[\"loc:@W_embedding\"], validate_indices=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](W_embedding/read, _arg_Placeholder_0_0)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m~/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    474\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[20411,2000,100]\n\t [[Node: embedding_lookup = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, _class=[\"loc:@W_embedding\"], validate_indices=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](W_embedding/read, _arg_Placeholder_0_0)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-103-04b6594f9c10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Hello\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mtest_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX_positive\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mX_test_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_negative\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mX_test_0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"OK\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;31m#test_f1 = f1.eval({X_positive:X_test_1, X_negative:X_test_0})\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, feed_dict, session)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m     \"\"\"\n\u001b[0;32m--> 570\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_eval_using_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_dup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_eval_using_default_session\u001b[0;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[1;32m   4453\u001b[0m                        \u001b[0;34m\"the tensor's graph is different from the session's \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4454\u001b[0m                        \"graph.\")\n\u001b[0;32m-> 4455\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1334\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1335\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1336\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1338\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[20411,2000,100]\n\t [[Node: embedding_lookup = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, _class=[\"loc:@W_embedding\"], validate_indices=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](W_embedding/read, _arg_Placeholder_0_0)]]\n\nCaused by op 'embedding_lookup', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/tasnim/tensorflow/lib/python3.5/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/tasnim/tensorflow/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/tasnim/tensorflow/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 478, in start\n    self.io_loop.start()\n  File \"/home/tasnim/tensorflow/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/tasnim/tensorflow/lib/python3.5/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/tasnim/tensorflow/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/tasnim/tensorflow/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/tasnim/tensorflow/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/tasnim/tensorflow/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/tasnim/tensorflow/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/tasnim/tensorflow/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 281, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/tasnim/tensorflow/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 232, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/tasnim/tensorflow/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 397, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/tasnim/tensorflow/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/tasnim/tensorflow/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/tasnim/tensorflow/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/tasnim/tensorflow/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/tasnim/tensorflow/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-86-cad23cf9c1e7>\", line 6, in <module>\n    score_positive, score_negative, parameters = forward_propagation(X_positive, X_negative, vocab, E, print_=False)\n  File \"<ipython-input-74-314e7ae23068>\", line 36, in forward_propagation\n    embedding_positive = tf.nn.embedding_lookup(W_embedding, X_positive)\n  File \"/home/tasnim/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/embedding_ops.py\", line 328, in embedding_lookup\n    transform_fn=None)\n  File \"/home/tasnim/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/embedding_ops.py\", line 150, in _embedding_lookup_and_transform\n    result = _clip(_gather(params[0], ids, name=name), ids, max_norm)\n  File \"/home/tasnim/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/embedding_ops.py\", line 54, in _gather\n    return array_ops.gather(params, ids, name=name)\n  File \"/home/tasnim/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/array_ops.py\", line 2486, in gather\n    params, indices, validate_indices=validate_indices, name=name)\n  File \"/home/tasnim/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 1834, in gather\n    validate_indices=validate_indices, name=name)\n  File \"/home/tasnim/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/tasnim/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"/home/tasnim/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[20411,2000,100]\n\t [[Node: embedding_lookup = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, _class=[\"loc:@W_embedding\"], validate_indices=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](W_embedding/read, _arg_Placeholder_0_0)]]\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "num_epochs = 1\n",
    "minibatch_size = 32\n",
    "m = num_train\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        minibatch_cost = 0.\n",
    "        num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
    "        #minibatches = mini_batches(X_train_1, X_train_0, m, minibatch_size)\n",
    "        minibatches = mini_batches(X_dev_1, X_dev_0, m, minibatch_size)\n",
    "\n",
    "        for (i, minibatch) in enumerate(minibatches):\n",
    "            if i == 10:\n",
    "                break\n",
    "            (minibatch_X_positive, minibatch_X_negative) = minibatch\n",
    "            \n",
    "            _ , temp_cost, pos, neg = sess.run([optimizer, cost, score_positive, score_negative], \n",
    "                        feed_dict={X_positive:minibatch_X_positive, \n",
    "                                   X_negative:minibatch_X_negative})\n",
    "            \"\"\"\n",
    "            print(\"Epoch:\", epoch, \"Minibatch:\", i) \n",
    "            print(\"Positive score:\")\n",
    "            print(pos) \n",
    "            print(\"Negative score:\")\n",
    "            print(neg)\n",
    "            print(\"ranking loss:\", temp_cost)\n",
    "            \n",
    "            print(\"*************** End of a minibatch **********************************\")\n",
    "            \"\"\"\n",
    "            print(\"Iteration \",i, \":  \",temp_cost)\n",
    "            minibatch_cost += temp_cost / num_minibatches\n",
    "        \n",
    "        #print(minibatch_cost)\n",
    "        print(\"******************* End of an epoch ******************************\")\n",
    "        wins = tf.greater(score_positive, score_negative)\n",
    "        number_wins = tf.reduce_mean(tf.cast(wins, \"float\"))\n",
    "        \n",
    "        ties = tf.equal(score_positive, score_negative)\n",
    "        number_ties = tf.reduce_mean(tf.cast(ties, \"float\"))\n",
    "\n",
    "        losses = tf.less(score_positive, score_negative)\n",
    "        number_losses = tf.reduce_mean(tf.cast(losses, \"float\"))\n",
    "\n",
    "        recall = number_wins/(number_wins + number_ties + number_losses)\n",
    "        \n",
    "        precision = number_wins/(number_wins+number_losses)\n",
    "\n",
    "        f1 = 2*precision*recall/(precision+recall)\n",
    "\n",
    "        accuracy = number_wins/(number_wins + number_ties + number_losses)\n",
    "        \n",
    "        \n",
    "        test_accuracy = accuracy.eval(feed_dict={X_positive:X_test_1, X_negative:X_test_0})\n",
    "        \n",
    "        test_f1 = f1.eval({X_positive:X_test_1, X_negative:X_test_0})\n",
    "        \n",
    "\n",
    "        print(\"Train Accuracy:\", test_accuracy)\n",
    "        print(\"Test Accuracy:\", test_f1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
